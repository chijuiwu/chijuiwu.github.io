<div class="container" id="content">

    <div id="about" class="row">
        <!-- <div class="col-md-10 offset-md-2"> -->
            <div class="col-md-2">
                <img class="avatar" alt="Chi-Jui Wu 吳啟瑞" src="/assets/chijuiwu-3.jpg">
            </div>

            <div class="col-md-10">
                <p class="text-justify">I am a <span class="badge badge-light badge-pill">technologist (data scientist)</span> by day and a <span class="badge badge-light badge-pill">humanist (writer)</span> by night. I work on natural language processing and human-AI collaoboration problems in the Business Intelligence Development Department at Taishin International Bank, Taiwan. Our team's projects have generated over 3 million NTD in annual profit. I read and write about technology and human life stories including biographies, life challenges, mental health, and positivity. I am particularly interested in human-centered computing, humanistic psychology, and education. My academic background is in computer science and machine learning. My research is in Human-Computer Interaction (HCI), which spans across cross-device interaction, proxemic interaction, and accessibility at St Andrews (undergraduate), UCL (masters), and Lancaster (PhD) in the UK as well as in Taiwan. I also spent one year as an English teaching assistant in Hualien, Taiwan.</p>
                <p>
                    <a href="/cv/CJW-CV.pdf" title="CV" aria-label="CV" target="_blank"><i class="fas fa-user-circle fa-2x" aria-hidden></i></a> &nbsp;
                    
                    <a href="https://medium.com/@chijuiwu" title="Medium" aria-label="Medium" target="_blank"><i class="fab fa-medium fa-2x" aria-hidden></i></a> &nbsp;

                    <a href="https://scholar.google.com/citations?user=EBXkRfoAAAAJ&hl=en" title="Google Scholar" aria-label="Google Scholar" target="_blank"><i class="ai ai-google-scholar-square ai-2x" aria-hidden></i></a>&nbsp;
        
                    <a href="https://github.com/chijuiwu" title="GitHub" aria-label="GitHub" target="_blank"><i class="fab fa-github-square fa-2x" aria-hidden></i></a> &nbsp; 
                    
                    <a href="https://twitter.com/chijuiwu" title="Twitter" aria-label="Twitter" target="_blank"><i class="fab fa-twitter-square fa-2x" aria-hidden></i></a> &nbsp;
                    
                    <a href="https://www.linkedin.com/in/chijuiwu/" title="LinkedIn" aria-label="LinkedIn" target="_blank"><i class="fab fa-linkedin fa-2x" aria-hidden></i></a> &nbsp;
                    
                    <a  href="mailto:charales@chijuiwu.space" title="Email" aria-label="Email" target="_blank"><i class="fas fa-envelope-square fa-2x" aria-hidden></i></a>
                </p>
            </div>
        <!-- </div> -->
    </div>

    <div id="recent" class="row section">
        <div class="col-md-12">

            <h4 class="display-4" class="section-title">Life Events</h4>

            <div class="bs-component">
                <span class="badge badge-light">2019 / 05 / 04</span> &nbsp;Published paper <a href="https://dl.acm.org/citation.cfm?id=3300792" title="Cross-Device Taxonomy paper" target="_blank">Cross-Device Taxonomy in CHI'19</a>.
            </div>

            <div class="bs-component">
                <span class="badge badge-light">2019 / 01 / 02</span> &nbsp;Started data scientist position at Taishin International Bank, Taiwan.
            </div>

            <div class="bs-component">
                <span class="badge badge-light">2018 / 01 / 15</span> &nbsp;Started PhD with fellowship at Interactive Systems, Lancaster University, UK. (Withdrawal from PhD study in August.)
            </div>

            <div class="bs-component">
                <span class="badge badge-light">2018 / 01 / 05</span> &nbsp;Completed military substitute service (English education) in Hualien, Taiwan.
            </div>

            <div class="bs-component">
                <span class="badge badge-light">2017 / 05 / 19</span> &nbsp;Writing on <a href="https://medium.com/@chijuiwu" title="Medium" target="_blank">Medium</a> about <span class="badge badge-light badge-pill">life stories and self-actualization</span>, <span class="badge badge-light badge-pill">well-being</span>, and <span class="badge badge-light badge-pill">technology</span>.
            </div>

            <div class="bs-component">
                <span class="badge badge-light">2017 / 05 / 06</span> &nbsp;Published paper <a href="https://dl.acm.org/citation.cfm?id=3025562" title="Paper - EagleSense" target="_blank">EagleSense in CHI'17</a>.
            </div>

            <div class="bs-component">
                <span class="badge badge-light">2016 / 12 / 02</span> &nbsp;Published paper <a href="https://link.springer.com/article/10.1007/s00779-016-0997-6" title="Paper - Out of Sight" target="_blank">Out of Sight in PUC</a>.
            </div>

            <div class="bs-component">
                <span class="badge badge-light">2016 / 11 / 01</span> &nbsp;Received MRes from University College London, UK (Distinction).
            </div>

            <div class="bs-component">
                <span class="badge badge-light">2015 / 06 / 24</span> &nbsp;Received BSc from University of St Andrews, UK (First-Class Honours).
            </div>
        </div>
    </div>


    <div id="writing" class="row section">
        <div class="col-md-12">

            <h4 class="display-4" class="section-title">Writing</h4>

            <p>A selected collection of my writing pieces:</p>

            <div class="bs-component">
                <img src="/blog/20190902_storytelling-and-scientific-reasoning/figure.jpg" class="img-fluid img-thumbnail writing-thumbnail" alt="storytelling-and-scientific-reasoning">
                <span class="badge badge-light">2019 / 09 / 02</span> &nbsp;<a href="https://medium.com/@chijuiwu/storytelling-and-scientific-reasoning-bb6982e87939" target="_blank" title="storytelling-and-scientific-reasoning" aria-label="storytelling-and-scientific-reasoning">Sam Harris and Jordan Peterson, on storytelling and scientific reasoning</a>
            </div>

            <div class="bs-component">
                <img src="/blog/20190707_coming-to-know-thyself/figure.jpg" class="img-fluid img-thumbnail writing-thumbnail" alt="coming-to-know-thyself">
                <span class="badge badge-light">2019 / 07 / 07</span> &nbsp;<a href="https://medium.com/@chijuiwu/coming-to-know-thyself-on-21-lessons-for-the-21st-century-by-yuval-noah-harari-68b87761b692" target="_blank" title="coming-to-know-thyself" aria-label="coming-to-know-thyself">Coming to know thyself, on 21 Lessons for the 21st Century by Yuval Noah Harari</a>
            </div>

            <div class="bs-component">
                <img src="/blog/20190420_thoughts-on-exploring-and-designing-for-memory-impairments-in-depression/figure.jpg" class="img-fluid img-thumbnail writing-thumbnail" alt="thoughts-on-exploring-and-designing-for-memory-impairments-in-depression">
                <span class="badge badge-light">2019 / 04 / 20</span> &nbsp;<a href="https://medium.com/@chijuiwu/thoughts-on-exploring-and-designing-for-memory-impairments-in-depression-by-chengcheng-qu-et-al-21c67cd5617c" target="_blank" title="thoughts-on-exploring-and-designing-for-memory-impairments-in-depression" aria-label="thoughts-on-exploring-and-designing-for-memory-impairments-in-depression">On Exploring and Designing for Memory Impairments in Depression by ChengCheng Qu et al.</a>
            </div>

            <div class="bs-component">
                <img src="/blog/20190414_alone-together-by-sherry-turkle/figure.jpg" class="img-fluid img-thumbnail writing-thumbnail" alt="alone-together-by-sherry-turkle">
                <span class="badge badge-light">2019 / 04 / 14</span> &nbsp;<a href="https://medium.com/@chijuiwu/thoughts-on-alone-together-by-sherry-turkle-7435212bdf84" target="_blank" title="alone-together-by-sherry-turkle" aria-label="alone-together-by-sherry-turkle">Thoughts on Alone Together by Sherry Turkle</a>
            </div>

            <div class="bs-component">
                <img src="/blog/20190210_psychology-of-life-stories/figure.jpg" class="img-fluid img-thumbnail writing-thumbnail" alt="psychology-of-life-stories">
                <span class="badge badge-light">2019 / 02 / 10</span> &nbsp;<a href="https://medium.com/@chijuiwu/psychology-of-life-stories-7e0f7075968" target="_blank" title="psychology-of-life-stories" aria-label="psychology-of-life-stories">生命故事心理學 (McAdams)</a>
            </div>

            <div class="bs-component">
                <img src="/blog/20171221_military-substitute-service/figure.jpg" class="img-fluid img-thumbnail writing-thumbnail" alt="英專替代役感想">
                <span class="badge badge-light">2017 / 12 / 21</span> &nbsp;<a href="https://medium.com/@chijuiwu/%E8%8B%B1%E5%B0%88%E6%9B%BF%E4%BB%A3%E5%BD%B9%E6%84%9F%E6%83%B3-98e0eb6316ae" target="_blank" title="英專替代役感想" aria-label="英專替代役感想">英專替代役感想</a>
            </div>

            <div class="bs-component">
                <img src="/blog/20170519_my-journal-to-chi-2017/figure.jpg" class="img-fluid img-thumbnail writing-thumbnail" alt="my-journal-to-chi-2017">
                <span class="badge badge-light">2017 / 05 / 19</span> &nbsp;<a href="https://medium.com/@chijuiwu/my-journey-to-chi-2017-e420dedc2cf5" target="_blank" title="my-journal-to-chi-2017" aria-label="my-journal-to-chi-2017">My Journal to CHI 2017</a>
            </div>
        </div>
    </div>



    <div id="work" class="row section">
        <div class="col-md-12">

            <h4 class="display-4" class="section-title">Work</h4>

            <p>A selected collection of my work projects:</p>

            <table class="table table-striped table-responsive table-sm">
                <tbody>

                <!-- STT -->
                <tr class="d-flex">
                    <td colspan="2">
                        <p class="research-title"><u>2019.</u> Taishin Speech-To-Text Team (<b>Role: Full-Stack Development</b>). <b>STT Search Engine.</b> Web Application.</p>
                    </td>
                </tr>
                <tr class="d-flex">
                    <td class="col-md-3">
                        <img src="/research/stt/stt-search-engine.jpg" alt="STT Search Engine" class="img-fluid img-thumbnail">
                    </td>
                    <td class="col-md-9">
                        <p class="text-justify research-description">A search engine and analytics platform for contact center conversations and unstructured big data. Its main technologies are: Elasticsearch, Docker, and Flask.</p>
                    </td>
                </tr>
                </tbody>
            </table>
        </div>
    </div>

    <div id="research" class="row section">
        <div class="col-md-12">

            <h4 class="display-4" class="section-title">Research</h4>

            <table class="table table-striped table-responsive table-sm">
                <tbody>

                <!-- Life Challenges -->
                <tr class="d-flex">
                    <td colspan="2">
                        <p class="research-title"><u>2019.</u> Ching Su and <b>Chi-Jui Wu</b>. <b>Weaving Human Experiences: Life Challenges in People with Visual Impairments.</b> Unpublished manuscript.</p>
                    </td>
                </tr>
                <tr class="d-flex">
                    <td class="col-md-3">
                        <img src="/research/life-story/life-sensitive-frame-web.jpg" alt="Life Sensitive Frame" class="img-fluid img-thumbnail">
                    </td>
                    <td class="col-md-9">
                        <p class="text-justify research-description">A growing interest within HCI is in understanding how people with visual impairments' disability emerges from interactions in situated, sociotechnical contexts. However, much of this work focuses on the accessibility experience in specific moments. In contrast, a person's whole life experience and life challenges have received little attention. We seek to weave the threads of accessibility research on diverse human experiences by exploring how technology, society, and culture jointly influence a person's growth throughout their lives. Human life is always changing, and our values and identity are reflected in our life stories. Through life story interviews with ten participants in Taiwan, we discuss three life challenges to personal growth in people with visual impairments and how technologies mediate the development of these life challenges. Finally, we introduce the life sensitive frame to support people with visual impairments as they grow and tackle their life challenges.</p>
                    </td>
                </tr>

                <!-- Cross-Device Taxonomy -->
                <tr class="d-flex">
                    <td colspan="2">
                        <p class="research-title"><u>2019.</u> Frederik Brudy, Christian Holz, Roman Rädle, <b>Chi-Jui Wu</b>, Steven Houben, Clemens Klokmose, and Nicolai Marquardt. <b>Cross-Device Taxonomy: Survey, Opportunities and Challenges of Interactions Spanning Across Multiple Devices.</b> In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (<a href="https://dl.acm.org/citation.cfm?id=3300792" target="_blank" title="acm paper" aria-label="acm paper">CHI'19</a>).</p>
                        <p style="margin:0px;">
                            <a href="https://youtu.be/cUSI5bQYP_8" target="_blank" title="video" aria-label="video"><i class="btn_media fab fa-youtube fa-2x" aria-hidden></i></a> &nbsp; 
                            <a href="/research/cross-device-taxonomy/cross-device-taxonomy-paper.pdf" target="_blank" title="paper" aria-label="paper"><i class="btn_media fas fa-file-pdf fa-2x"></i></a>
                        </p>
                    </td>
                </tr>
                <tr class="d-flex">
                    <td class="col-md-3">
                        <img src="/research/cross-device-taxonomy/cross-device-taxonomy-paper-preview-figure.jpg" alt="Cross-Device Taxonomy" class="img-fluid img-thumbnail">
                    </td>
                    <td class="col-md-9">
                        <p class="text-justify research-description">Designing interfaces or applications that move beyond the bounds of a single device screen enables new ways to engage with digital content. Research addressing the opportunities and challenges of interactions with multiple devices in concert is of continued focus in HCI research. To inform the future research agenda of this field, we contribute an analysis and taxonomy of a corpus of 510 papers in the cross-device computing domain. For both new and experienced researchers in the field we provide: an overview, historic trends and unified terminology of cross-device research; discussion of major and under-explored application areas; mapping of enabling technologies; synthesis of key interaction techniques spanning across multiple devices; and review of common evaluation strategies. We close with a discussion of open issues. Our taxonomy aims to create a unified terminology and common understanding for researchers in order to facilitate and stimulate future cross-device research.</p>
                    </td>
                </tr>
                    
                <!-- EagleSense -->
                <tr class="d-flex">
                    <td colspan="2">
                        <p class="research-title"><u>2017.</u> <b>Chi-Jui Wu</b>, Steven Houben, and Nicolai Marquardt. <b>EagleSense: Tracking People and Devices in Interactive Spaces using Real-Time Top-View Depth-Sensing.</b> In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (<a href="https://dl.acm.org/citation.cfm?id=3025562" target="_blank" title="acm paper" aria-label="acm paper">CHI'17</a>).</p>
                        <p style="margin:0px;">
                            <a href="https://youtu.be/6QLHA7hC_Kc" target="_blank" title="video" aria-label="video"><i class="btn_media fab fa-youtube fa-2x" aria-hidden></i></a> &nbsp; 
                            <a href="/research/eaglesense/eaglesense-paper.pdf" target="_blank" title="paper" aria-label="paper"><i class="btn_media fas fa-file-pdf fa-2x"></i></a> &nbsp; 
                            <a href="/research/eaglesense/eaglesense-slides.pptx" target="_blank" title="slides" aria-label="slides"><i class="btn_media fab fa-slideshare fa-2x"></i></a> &nbsp; 
                            <a href="https://github.com/chijuiwu/eaglesense" target="_blank" title="code" aria-label="code"><i class="btn_media fab fa-github fa-2x" aria-hidden></i></a>
                        </p>
                    </td>
                </tr>
                <tr class="d-flex">
                    <td class="col-md-3">
                        <img src="/research/eaglesense/eaglesense-web.jpg" alt="EagleSense" class="img-fluid img-thumbnail">
                    </td>
                    <td class="col-md-9">
                        <p class="text-justify research-description">Real-time tracking of people's location, orientation and activities is increasingly important for designing novel ubiquitous computing applications. Top-view camera-based tracking avoids occlusion when tracking people while collaborating, but often requires complex tracking systems and advanced computer vision algorithms. To facilitate the prototyping of ubiquitous computing applications for interactive spaces, we developed EagleSense, a real-time human posture and activity recognition system with a single top-view depth-sensing camera. We contribute our novel algorithm and processing pipeline, including details for calculating silhouette-extremities features and applying gradient tree boosting classifiers for activity recognition optimized for top-view depth sensing. EagleSense provides easy access to the real-time tracking data and includes tools for facilitating the integration into custom applications. We report the results of a technical evaluation with 12 participants and demonstrate the capabilities of EagleSense with application case studies.</p>
                    </td>
                </tr>
        

                <!-- Out of Sight -->
                <tr class="d-flex">
                    <td colspan="2">
                        <p class="research-title"><u>2017.</u> <b>Chi-Jui Wu</b>, Aaron Quigley, and David Harris-Birtill. <b>Out of Sight: A Toolkit for Tracking Occluded Human Joint Positions.</b> In Personal and Ubiquitous Computing (<a href="https://link.springer.com/article/10.1007/s00779-016-0997-6" target="_blank" title="springer paper" aria-label="springer paper">PUC'17</a>).</p>
                        <p style="margin:0px;">
                            <a href="https://youtu.be/gUs8tlfv_tQ" target="_blank" title="video" aria-label="video"><i class="fab fa-youtube fa-2x" aria-hidden></i></a> &nbsp; 
                            <a href="/research/out-of-sight/out-of-sight-paper.pdf" target="_blank" title="paper" aria-label="paper"><i class="fas fa-file-pdf fa-2x"></i></a> &nbsp; 
                            <a href="https://github.com/chijuiwu/kinect2kit" target="_blank" title="code" aria-label="code"><i class="fab fa-github fa-2x" aria-hidden></i></a>
                        </p>
                    </td>
                </tr>
                <tr class="d-flex">
                    <td class="col-md-3">
                        <img src="/research/out-of-sight/out-of-sight-web.jpg" alt="Out of Sight" class="img-fluid img-thumbnail">
                    </td>
                    <td class="col-md-9">
                        <p class="text-justify research-description">Real-time identification and tracking of the joint positions of people can be achieved with off-the-shelf sensing technologies such as the Microsoft Kinect, or other camera-based systems with computer vision. However, tracking is constrained by the system’s field of view of people. When a person is occluded from the camera view, their position can no longer be followed. Out of Sight addresses the occlusion problem in depth-sensing tracking systems. Our new tracking infrastructure provides human skeleton joint positions during occlusion, by combining the field of view of multiple Kinects using geometric calibration and affine transformation. We verified the technique’s accuracy through a system evaluation consisting of 20 participants in stationary position and in motion, with two Kinects positioned parallel, 45&#176;, and 90&#176; apart. Results show that our skeleton matching is accurate to within 16.1 cm (s.d. = 5.8 cm), which is within a person’s personal space. In a realistic scenario study, groups of two people quickly occlude each other, and occlusion is resolved for   85%  of the participants. A RESTful API was developed to allow distributed access of occlusion-free skeleton joint positions. As a further contribution, we provide the system as open source.</p>
                    </td>
                </tr>
                </tbody>
            </table>
        </div>
    </div>


    <div id="updates" class="row" style="margin-top:100px; margin-bottom: 50px; font-family:Verdana;font-size:12px;">
        <div class="col-md-8 col-md-offset-2">
            <p class="text-justify"><i class="btn_media far fa-copyright fa-1x" aria-hidden></i> Chi-Jui Wu 吳啟瑞 | Last Updated: 2020/02/09</p>
        </div>
    </div>
</div>