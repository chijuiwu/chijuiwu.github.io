<div class="container">
    
    <h1 class="display-4">Publications</h1>
    
    <table class="table table-striped table-responsive" style="font-size:14px; font-family:Helvetica; color:black;">
        <tbody>
        <tr>
            <td style="width:30%">
                <img src="/research/eaglesense/eaglesense-figure1-new.png" class="center-block img-responsive">
            </td>
            <td style="width:70%">
                <p><strong>EagleSense: Tracking People and Devices in Interactive Spaces using Real-Time Top-View Depth-Sensing</strong></p>
                
                <p>
                    <a href="/research/eaglesense/eaglesense-paper.pdf" target="_blank" title="paper"><i class="fa fa-file-pdf-o fa-2x" aria-hidden="true"></i></a> | 
                    <a href="https://youtu.be/6QLHA7hC_Kc" target="_blank" title="video"><i class="fa fa-youtube fa-2x" aria-hidden="true"></i></a> | 
                    <a href="/research/eaglesense/eaglesense-slides.pptx" target="_blank" title="powerpoint"><i class="fa fa-slideshare fa-2x" aria-hidden="true"></i></a> | 
                    <a href="https://github.com/cjw-charleswu/eaglesense" target="_blank" title="code"><i class="fa fa-github fa-2x" aria-hidden="true"></i></a>
                </p>

                <p>Chi-Jui Wu, Steven Houben, and Nicolai Marquardt.</p>
                
                <p><u>2017.</u> In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI'17).</p>
                
                <p style="font-size:12px;">Real-time tracking of people's location, orientation and activities is increasingly important for designing novel ubiquitous computing applications. Top-view camera-based tracking avoids occlusion when tracking people while collaborating, but often requires complex tracking systems and advanced computer vision algorithms. To facilitate the prototyping of ubiquitous computing applications for interactive spaces, we developed EagleSense, a real-time human posture and activity recognition system with a single top-view depth-sensing camera...</p>
            </td>
        </tr>

        <tr>
            <td style="width:30%">
                <img src="/research/out-of-sight/out-of-sight-figure1.png" class="center-block img-responsive">
            </td>
            <td style="width:70%">
                <p><strong>Out of Sight: A Toolkit for Tracking Occluded Human Joint Positions</strong></p>
                
                <p>
                    <a href="/research/out-of-sight/out-of-sight-paper.pdf" target="_blank" title="paper"><i class="fa fa-file-pdf-o fa-2x" aria-hidden="true"></i></a> | 
                    <a href="https://youtu.be/gUs8tlfv_tQ" target="_blank" title="video"><i class="fa fa-youtube fa-2x" aria-hidden="true"></i></a> | 
                    <!--<a href="/research/eaglesense/eaglesense-slides.pptx" target="_blank" title="powerpoint"><i class="fa fa-slideshare fa-2x" aria-hidden="true"></i></a> | -->
                    <a href="https://github.com/cjw-charleswu/kinect2kit" target="_blank" title="code"><i class="fa fa-github fa-2x" aria-hidden="true"></i></a>
                </p>

                <p>Chi-Jui Wu, Aaron Quigley, and David Harris-Birtill.</p>
                
                <p><u>2017.</u> In Personal and Ubiquitous Computing.</p>
                
                <p style="font-size:12px;">Real-time identification and tracking of the joint positions of people can be achieved with off-the-shelf sensing technologies such as the Microsoft Kinect, or other camera-based systems with computer vision. However, tracking is constrained by the systemâ€™s field of view of people. When a person is occluded from the camera view, their position can no longer be followed. Out of Sight addresses the occlusion problem in depth-sensing tracking systems...</p>
            </td>
        </tr>
        
        </tbody>
    </table>

    <!-- <h1 class="display-4">More Research Work</h1> -->
    
</div>
