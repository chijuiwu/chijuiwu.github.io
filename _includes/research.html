<div class="container">

    <div id="research" class="row" style="font-size:14px; font-family:Helvetica; color:black;">
        <div class="col-md-8 col-md-offset-2">

            <h1 class="display-4" style="margin-top:0px; margin-bottom:20px;">Publications</h1>

            <table class="table table-striped table-responsive">
                <tbody>
                <tr>
                    <td style="width:30%">
                        <img src="/research/eaglesense/eaglesense-figure1-new.png" class="center-block img-responsive">
                        <p style="margin-top: 20px;">
                            <a href="https://youtu.be/6QLHA7hC_Kc" target="_blank" title="video" aria-label="video"><i class="fab fa-youtube fa-2x" aria-hidden></i></a> 
                            <a href="/research/eaglesense/eaglesense-paper.pdf" target="_blank" title="paper" aria-label="paper"><i class="fas fa-file-pdf fa-2x"></i></a> 
                            <a href="/research/eaglesense/eaglesense-slides.pptx" target="_blank" title="slides" aria-label="slides"><i class="fab fa-slideshare fa-2x"></i></a>  
                            <a href="https://github.com/chijuiwu/eaglesense" target="_blank" title="code" aria-label="code"><i class="fab fa-github fa-2x" aria-hidden></i></a>
                        </p>
                    </td>
                    <td style="width:70%">
                        <p><b>Chi-Jui Wu</b>, Steven Houben, and Nicolai Marquardt.</p>
                        
                        <p><b><u>2017.</u></b> EagleSense: Tracking People and Devices in Interactive Spaces using Real-Time Top-View Depth-Sensing. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI'17).</p>
                        
                        <p class="text-justify" style="font-size:11px;">Real-time tracking of people's location, orientation and activities is increasingly important for designing novel ubiquitous computing applications. Top-view camera-based tracking avoids occlusion when tracking people while collaborating, but often requires complex tracking systems and advanced computer vision algorithms. To facilitate the prototyping of ubiquitous computing applications for interactive spaces, we developed EagleSense, a real-time human posture and activity recognition system with a single top-view depth-sensing camera. We contribute our novel algorithm and processing pipeline, including details for calculating silhouette-extremities features and applying gradient tree boosting classifiers for activity recognition optimized for top-view depth sensing. EagleSense provides easy access to the real-time tracking data and includes tools for facilitating the integration into custom applications. We report the results of a technical evaluation with 12 participants and demonstrate the capabilities of EagleSense with application case studies.</p>
                    </td>
                </tr>
        
                <tr>
                    <td style="width:30%">
                        <img src="/research/out-of-sight/out-of-sight-figure1.png" class="center-block img-responsive">
                        <p style="margin-top: 20px;">
                            <a href="https://youtu.be/gUs8tlfv_tQ" target="_blank" title="video" aria-label="video"><i class="fab fa-youtube fa-2x" aria-hidden></i></a> 
                            <a href="/research/out-of-sight/out-of-sight-paper.pdf" target="_blank" title="paper" aria-label="paper"><i class="fas fa-file-pdf fa-2x"></i></a> 
                            <a href="https://github.com/chijuiwu/kinect2kit" target="_blank" title="code" aria-label="code"><i class="fab fa-github fa-2x" aria-hidden></i></a>
                        </p>
                    </td>
                    <td style="width:70%">
                        <p><b>Chi-Jui Wu</b>, Aaron Quigley, and David Harris-Birtill.</p>
                        
                        <p><b><u>2017.</u></b> Out of Sight: A Toolkit for Tracking Occluded Human Joint Positions. In Personal and Ubiquitous Computing.</p>

                        <p class="text-justify" style="font-size:11px;">Real-time identification and tracking of the joint positions of people can be achieved with off-the-shelf sensing technologies such as the Microsoft Kinect, or other camera-based systems with computer vision. However, tracking is constrained by the system’s field of view of people. When a person is occluded from the camera view, their position can no longer be followed. Out of Sight addresses the occlusion problem in depth-sensing tracking systems. Our new tracking infrastructure provides human skeleton joint positions during occlusion, by combining the field of view of multiple Kinects using geometric calibration and affine transformation. We verified the technique’s accuracy through a system evaluation consisting of 20 participants in stationary position and in motion, with two Kinects positioned parallel, 45&#176;, and 90&#176; apart. Results show that our skeleton matching is accurate to within 16.1 cm (s.d. = 5.8 cm), which is within a person’s personal space. In a realistic scenario study, groups of two people quickly occlude each other, and occlusion is resolved for   85%  of the participants. A RESTful API was developed to allow distributed access of occlusion-free skeleton joint positions. As a further contribution, we provide the system as open source.</p>
                    </td>
                </tr>
                
                </tbody>
            </table>
        </div>
    </div>

    
</div>
